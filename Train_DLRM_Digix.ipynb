{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Train_DLRM_Digix.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN/opfjD8apcfinAX3OAhwK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fWXJY0QQ5fuE"},"source":["# Training Facebook's DLRM on the digix dataset "]},{"cell_type":"markdown","metadata":{"id":"DdaJ5jhn5xeZ"},"source":["Run this notebook on google colab and select GPU as runtime. Follow the steps below in order to run the Digix API"]},{"cell_type":"markdown","metadata":{"id":"YaOJbOXL7ijM"},"source":["optionally mount your google drive account to persist the data outside of the current runime"]},{"cell_type":"code","metadata":{"id":"WWFO4S6y5x1i"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kotvo5_B53Zh"},"source":["import os\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AMM_PEog7hrw"},"source":["base_path = \"/content/drive/MyDrive/Colab Notebooks/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vm2lkQus74fm"},"source":["or just run it on this machine only\n","\n"]},{"cell_type":"code","metadata":{"id":"qVrQrvKP78X3","executionInfo":{"status":"ok","timestamp":1611401487821,"user_tz":-60,"elapsed":531,"user":{"displayName":"Max Beckers","photoUrl":"","userId":"16118706251397883059"}}},"source":["base_path = \"/content/\""],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"C5QHq0aJ57Cf","executionInfo":{"status":"ok","timestamp":1611401488942,"user_tz":-60,"elapsed":649,"user":{"displayName":"Max Beckers","photoUrl":"","userId":"16118706251397883059"}}},"source":["!mkdir dlrm_facebook"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMpTU0AP6u8n","executionInfo":{"status":"ok","timestamp":1611401491087,"user_tz":-60,"elapsed":529,"user":{"displayName":"Max Beckers","photoUrl":"","userId":"16118706251397883059"}}},"source":["import os\n","os.chdir(base_path + \"dlrm_facebook\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qsy7tgmE7E9V","executionInfo":{"status":"ok","timestamp":1611401509001,"user_tz":-60,"elapsed":14091,"user":{"displayName":"Max Beckers","photoUrl":"","userId":"16118706251397883059"}},"outputId":"a503f182-947c-4750-a8af-922e6f20dc4a"},"source":["# donwload the digix dataset from kaggle \n","os.environ['KAGGLE_USERNAME'] = \"<your username>\"\n","os.environ['KAGGLE_KEY'] = \"<your kaggle key>\"\n","!kaggle datasets download -d louischen7/2020-digix-advertisement-ctr-prediction"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading 2020-digix-advertisement-ctr-prediction.zip to /content/dlrm_facebook\n","100% 1.20G/1.20G [00:12<00:00, 116MB/s]\n","100% 1.20G/1.20G [00:12<00:00, 100MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rTzZ7cdQ67ze","executionInfo":{"status":"ok","timestamp":1611401669957,"user_tz":-60,"elapsed":167541,"user":{"displayName":"Max Beckers","photoUrl":"","userId":"16118706251397883059"}}},"source":["import json\n","import zipfile\n","\n","zip_ref = zipfile.ZipFile(\"2020-digix-advertisement-ctr-prediction.zip\", 'r')\n","zip_ref.extractall()\n","zip_ref.close()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h1zS0cWD9vpL","executionInfo":{"status":"ok","timestamp":1611408500131,"user_tz":-60,"elapsed":560,"user":{"displayName":"Max Beckers","photoUrl":"","userId":"16118706251397883059"}},"outputId":"3730d91f-225d-43ee-f5fb-db50a058b384"},"source":["!ls"],"execution_count":39,"outputs":[{"output_type":"stream","text":["2020-digix-advertisement-ctr-prediction.zip  test_data_A.csv  train_data\n","dlrm\t\t\t\t\t     test_data_B.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yayCm5mt6oq6","executionInfo":{"status":"ok","timestamp":1611401679694,"user_tz":-60,"elapsed":1198,"user":{"displayName":"Max Beckers","photoUrl":"","userId":"16118706251397883059"}},"outputId":"b38de250-f3f7-4e2d-a6aa-19a165b5b1aa"},"source":["!git clone https://github.com/mabeckers/dlrm.git"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Cloning into 'dlrm'...\n","remote: Enumerating objects: 20, done.\u001b[K\n","remote: Counting objects: 100% (20/20), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 511 (delta 11), reused 14 (delta 6), pack-reused 491\u001b[K\n","Receiving objects: 100% (511/511), 1.32 MiB | 30.00 MiB/s, done.\n","Resolving deltas: 100% (300/300), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AE_2eviH6z4m","executionInfo":{"status":"ok","timestamp":1611401681167,"user_tz":-60,"elapsed":489,"user":{"displayName":"Max Beckers","photoUrl":"","userId":"16118706251397883059"}}},"source":["os.chdir(base_path + \"dlrm_facebook/dlrm\")"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0H7BDne661fS","executionInfo":{"status":"ok","timestamp":1611401682638,"user_tz":-60,"elapsed":552,"user":{"displayName":"Max Beckers","photoUrl":"","userId":"16118706251397883059"}},"outputId":"eafd0727-3e89-430b-cde8-fba07787f548"},"source":["!git checkout new_dataset"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Branch 'new_dataset' set up to track remote branch 'new_dataset' from 'origin'.\n","Switched to a new branch 'new_dataset'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xclicz6V-KP7","executionInfo":{"status":"ok","timestamp":1611408511339,"user_tz":-60,"elapsed":547,"user":{"displayName":"Max Beckers","photoUrl":"","userId":"16118706251397883059"}}},"source":["!python dlrm_s_pytorch.py --data-generation=dataset --mini-batch-size=1000 --data-set=digix --raw-data-file=\"/content/dlrm_facebook/train_data/train_data.csv\" --processed-data-file=\"/content/dlrm_facebook/dlrm/digix_processed.npz\" --nepochs=50 --print-freq=4000 --test-freq=4000 --loss-function=bce --test-mini-batch-size=2000 --arch-sparse-feature-size=5 --arch-mlp-bot=\"8-5\" --arch-mlp-top=\"150-75-1\" --learning-rate=0.01 --use-gpu"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4S9V0GW7XeU"},"source":["# run this if you have mounted google colab\n","#!python dlrm_s_pytorch.py --data-generation=dataset --mini-batch-size=1000 --data-set=digix --raw-data-file=\"/content/drive/MyDrive/Colab Notebooks/dlrm_facebook/ctr-prediction-digix/train_data/train_data.csv\" --processed-data-file=\"/content/drive/MyDrive/Colab Notebooks/dlrm_facebook/ctr-prediction-digix/train_data/digix_processed.npz\" --nepochs=30 --print-freq=4000 --test-freq=4000 --loss-function=bce --test-mini-batch-size=2000 --arch-sparse-feature-size=5 --arch-mlp-bot=\"8-5\" --arch-mlp-top=\"150-75-1\" --learning-rate=0.01 --use-gpu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-_JpaoT3D2b1","executionInfo":{"status":"ok","timestamp":1611408522939,"user_tz":-60,"elapsed":909,"user":{"displayName":"Max Beckers","photoUrl":"","userId":"16118706251397883059"}}},"source":["%load_ext tensorboard"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"EQAiEIizFys5","executionInfo":{"status":"ok","timestamp":1611408524550,"user_tz":-60,"elapsed":537,"user":{"displayName":"Max Beckers","photoUrl":"","userId":"16118706251397883059"}}},"source":["%tensorboard --logdir=\"/content/dlrm_facebook/dlrm/run_kaggle_pt\""],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dun5K__3D27e"},"source":["# Notes\n","\n","1. The goal of this project was to create a simple and technically clean extension of the DLRM model API that can use the raw digix data as input.\n","\n","2. The model is in no way at the hight of its possibilities yet when it comes to performance! Things that can be improved to get better performance:\n","- Data cleaning! Outlier detection and removal of unwanted features / unrealistic rows etc.\n","- Potentially removing certain users that do not contribute enough (e.g. don't have enough 1s)\n","- Potentially removing certain ad campaigns that aren't successfull enough. \n","- Feature engineering. In general understanding the data and the CTR prediction in the digix context a lot better.\n","- Changing and tuning the hyperparameters of the model such as the embedding size as well as the top and bottom MLP architecture \n","- Try out different under / over sampling techniques\n","- Use the full dataset\n","- change the training mechanism. Change LR scheduler and or the optimization algorithm (e.g. Stochastic gradient descent with restarts and cosine annealing for learing rate).\n","- Add more regularization! In the above Tensorflow logs of the last traning run it is clear to see that overfitting happens around the 40Kth training iteration and that we should add some form of regularization to work against that\n","\n","3. It is important to take a look at label distribution in this dataset as there is a huge imbalance between the positive and the negative class (way more negative). As one would assume of a real world dataset. In order to fix this huge imbalance, I did 2 things.\n","- Toss out all users that only have negative labels\n","- Undersample the remaining dataset such that 60% 0s and 40% 1s remain. \n","\n","\n","3. In my trials google colab was not able to handle the full (around 40 million rows) of this dataset, which is why I only used half. \n","\n"]}]}